#Learning What and Where to Transfer(学习迁移什么和迁移到哪）  
######预计讲解时间 25 mins
##摘要
随着深度学习的应用扩展到训练数据量不足的现实问题，迁移学习作为这种小数据体制下提高性能的手段，近年来受到了广泛的关注。然而，当现有的方法应用于异构体系结构和任务时，如何管理它们的详细配置就变得更加重要，并且常常需要对它们进行调优以获得所需的性能。  
为了解决这一问题，作者提出了一种新的基于**元学习**的迁移学习方法，该方法可以自动学习从源网络到目标网络迁移什么知识。在给定源网络和目标网络的情况下，提出了一种学习元网络的有效训练方案，该方案可以决定：  
(a)源网络和目标网络之间的哪些层应该匹配以进行知识迁移  
(b)哪些特征以及每个特征中的知识迁移量。  
针对各种数据集和网络架构上最近的迁移学习方法验证了作者的元迁移方法，在这些数据集和网络架构上，本文自动化方案显著优于以往。  
##引言
深度神经网络 (DNN) 需要大型数据集，但为每个目标任务收集足够数量的标记样本非常昂贵。处理这种数据缺乏的一种常用方法是迁移学习，其目标是将知识从已知源任务迁移到新目标任务。  
最广泛使用的迁移学习方法是使用微调进行**预训练**：首先使用大型数据集，然后，使用学习到的权重作为初始化来训练目标 DNN。然而，微调不是万能的，如果源任务和目标任务在语义上相差甚远，就可能没有任何好处。  
Cui等人建议根据预训练的目标任务从源数据集中采样，但这只有在源数据集可用时才有效。如果源任务和目标任务的网络架构大不相同，没有直接的方法来进行微调。
###动机
作者的动机是，这些方法虽然允许在异构源和目标任务/架构之间传输知识，但没有机制来识别要在网络的**哪些层**之间传输哪些源信息。  
根据任务的不同，一些源信息比其他的更重要，而另一些则无关紧要甚至起反作用。  
此外，在 *异构网络架构* 下，将源网络中的层与目标网络中的层关联起来并不简单。由于没有机制来学习将什么迁移到哪里，现有的方法需要根据任务仔细 **手动配置** 源网络和目标网络之间的层关联，这不是最优的。
###贡献
为了解决这个问题，作者提出了一种**基于元学习概念的新型迁移学习方法**，该方法学习从源网络到具有异构架构的目标网络将哪些信息传输到何处。作者的目标是学习以**自动方式**进行知识的迁移，考虑到源和目标之间的架构和任务的差异，无需手动调整迁移配置。  
具体来说，作者学习每个特征以及每对源层和目标层之间以及目标网络生成权重的元网络。因此，它可以自动学习识别哪些源网络知识有用，以及它应该迁移到哪里。提出了一种全新的方法 **learning to transfer what and where (L2T-ww)**  
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\图1.png) 
 
- 上图：先前的方法。两个网络之间的知识传输是在手工选择的层对之间完成的，而不考虑通道的重要性。  
 下图：元传输方法。元网络 f、g 自动决定两个网络层之间的知识迁移量和迁移时通道的重要性。线宽表示传输层和通道对中的传输量。

作者的**贡献**如下：  
**1** 作者引入了用于迁移学习的元网络，它可以自动决定源模型的哪些特征图（通道）对学习目标任务有用且相关，以及哪些源层应该迁移到哪些目标层。  
**2** 为了学习元网络的参数，作者提出了一个有效的元学习方案。作者主要的新奇之处在于评估一个目标模型的单步适应性能(元目标)，这个目标模型是通过最小化传递目标(作为一个内部目标)来学习的。与标准方案相比，该方案显著加快了内环过程。  
**3** 在作者的实验中，该方法比基线迁移学习方法有了显著的改进。例如，在ImageNet实验中，作者的元迁移学习学习方法在CUB200上的正确率为65.05%，而第二好的基线正确率为58.90%。特别是，当目标任务的训练样本数量不足和从多个源模型迁移时，作者的方法与基线方法相比有较大的优势。
##方法
作者的目标是学习将有用的知识从源网络传输到目标网络，而不需要手动层关联或特征选择。为此，作者提出了一种元学习方法，学习将源网络的哪些知识传输到目标网络中的哪一层。在本文中，作者主要关注卷积神经网络之间的迁移学习，但作者的方法是通用的，也适用于其他类型的深度神经网络。
###加权特征匹配
如果卷积神经网络在一项任务上训练有素，那么它的中间特征空间应该具有对该任务有用的知识。因此，模仿训练有素的特征可能有助于训练另一个网络。  

设 x 为输入，y 为相应的输出。令 Sm(x) 为预训练源网络 S 的第 m 层的中间特征映射(feature map）。目的是利用 S 的知识训练目标域的待学习网络Tθ。则T nθ (x) 表示目标域网络中第 n 层的特征映射(feature map）。作者想要在训练目标模型时，目标模型第 n 层输出的feature maps与预训练源模型的第 m 层输出的feature maps的L2 Loss最小化。  
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\L2.png)  
  
- 其中 ||·||F 指的是Frobenius范数,定义为矩阵A各项元素的绝对值平方的总和开根。（范数实际上就是衡量这个矩阵和对应的零矩阵的距离，就像二维平面上的一个点，和原点的距离就是它的f范数。所以希望范数足够的小。）

	- L0范数：**表示向量 x 中非零元素的个数。**
	- L1范数：**表示向量 x 中的绝对值之和。**L1范数有好多名字，例如：我们熟悉的曼哈顿距离、最小绝对误差等。
	- L2范数：**表示向量 x 的各元素平方和后再开方。** ℓ2​范数是我们最常见的范数。比如欧氏距离（Euclid distance，欧几里得距离）就是一种ℓ2​范数。
	- L∞范数：**度量向量中绝对值的最大值。** 


>   
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\范数.png)  

- 上式是总的**学习目标**，表示了在源域的第 m 层和目标域第 n 层之间迁移多少。 

其中rθ一个线性变换，例如逐点卷积。作者将这种方法称为**特征匹配**。这里，参数θ由线性变换参数 rθ 和非线性神经网络 Tθ 组成。

####迁移什么（What to transfer）
在一般的迁移学习设置中，目标模型针对与源模型不同的任务进行训练。在这种情况下，并非源模型的所有中间特征都可能对学习目标任务有用。  
因此，为了更多地关注有用的通道，作者考虑了一个**加权特征匹配损失**，作者对特征图的通道进行加权，得到加权特征匹配损失：  

![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\Lwhat.png)  

其中 H × W 是 Sm(x) 和 rθ(T nθ (x)) 的空间大小，Wm,nc 是通道 c 的非负权重，且∑c wm,nc = 1（欧米伽）。  
由于要传输的重要通道对于每个输入图像可能不同，作者将通道权重设置为函数（1），通过将源模型的特征作为输入的小型元网络的 softmax 输出。在本文中，作者让 φ （fai）表示元网络的参数，其参数 φ 就是学习目标  

- ![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\Lwhat解释.png)  
- Softmax函数一般作为神经网络的最后一层，接受来自上一层网络的输入值，然后将其转化为概率。Softmax函数可以将上一层的原始数据进行归一化，转化为一个(0,1)之间的数值，这些数值可以被当做概率分布，用来作为多分类的目标预测值。
####迁移到哪（Where to transfer）
将知识从源模型迁移到目标模型时，决定源模型和目标模型中的层对 (m, n) 对其有效性至关重要。以前的方法（Romero 等人，2015 年；Zagoruyko 和 Komodakis，2017 年）根据架构的先验知识或任务之间的语义相似性**手动选择**。  
但是，当源域目标域网络异构时（比如从RenNet迁移到VGG），要找到最优解很困难，常常需要小心的调优。因此引入了一个可学习的参数权重矩阵λm,n ≥ 0（兰布达），来表示源域中第m层对于目标域中第n层的可迁移指标。此指标越大，则越可迁移。  
与what同理，作者将其参数化为一个待学习网络：
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\元网络g.png)  
给定通道权重 w （欧米伽）和匹配对权重 λ（兰布达）的组合传输损失是：  
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\Lwfm.png)  
其中 C 是一组候选对，训练目标模型的**最终损失 L total** 如下：
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\Ltotal.png)  
其中 Lorg 是原始损失（例如交叉熵），β > 0 是超参数。作者注意到 Wm,n 和 λm,n 分别决定**迁移什么**和**迁移到哪里**  

>  对应train_L2T_ww.py 第268行**def inner_objective(data, matching_only=False):**

***整体的网络结构***如下图所示：
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\总.png)  
虚线表示特征图等张量流，实线表示L2特征匹配。  
(a) g m,n φ 分别输出源模型和目标模型的第 m 层和第 n 层之间的匹配对 λm,n 的权重  
(b) f m,n φ 输出每个通道的权重。

###训练元网络和目标模型
作者的目标是在使用训练目标 Ltotal(·|x, y, φ) 学习目标模型时在目标任务上实现高性能。为了最大化性能，特征匹配项 Lwfm(·|x, φ) 应该鼓励学习对目标任务有用的特征，例如预测标签。   
为了测量和增加由 φ 参数化的元网络决定的特征匹配的有用性，**标准方法**是使用以下双层方案来训练φ：  
**1** 更新T次θ来最小化 Ltotal(θ|x,y,ϕ);  
**2** 测量 Lorg(θ|x, y) 并更新 φ 以使其最小化。  
内循环实际采用的损失为 Ltotal(θ|x,y,ϕ)，而 Lorg用于衡量学习到的目标模型的有效性。但由于这个方法中元网络是通过 Lwfm这个正则化项微弱地对目标模型的学习过程产生影响，所以除非使用大量的内循环迭代，不然使用梯度 ∇ϕLorg来更新 ϕ是很困难的。  
于是，作者提出了一种交替更新的策略：  
**1** 更新 T次 θ来最小化 Lwfm(θ|x,ϕ);  
  
- 给定当前参数 θ0=θ，通过最小化 Lwfm来更新目标模型 T次，这时网络参数为 θT。实现 T 次**特征匹配**，更新θ使得目标模型学到从源模型迁移的知识，从而加强了元网络的影响，解决了上述的问题；
  
**2** 更新一次θ来最小化 Lorg(θ|x,y);   
  
- 通过目标标签进行一步自适应，网络参数从 θT更新为 θT+1。实现总**误差最小化**；   

**3** 测量Lorg(θ|x,y)并更新 ϕ来最小化它。  
  
- 利用Lorg(θT+1)来测量第一阶段和第二阶段使用的样本下目标模型适应目标任务的情况，然后通过更新元参数 ϕ来最小化 Lorg(θT+1)这样可以加速对 ϕ的训练。更新元神经网络的参数使得目标模型更好。文中的T取2。   


##实验
###安排
源和目标的网络架构和任务。为了评估包括作者在内的各种迁移学习方法，作者对图像分类任务的两个尺度 32 × 32 和 224 × 224 进行了实验。  
对于 32 × 32 尺度，作者使用 TinyImageNet1 数据集作为源任务，CIFAR-10、CIFAR-100和 STL-10 数据集作为目标任务。作者分别在源任务和目标任务上训练 32 层 ResNet和 9 层 VGG。  
对于 224×224 尺度，ImageNet数据集被用作源数据集，Caltech-UCSD Bird 200, MIT Indoor Scene Recognition, Stanford 40 个 Actions和 Stanford Dogs数据集作为目标任务。对于这些数据集，除非另有说明，否则分别使用 34 层和 18 层 ResNet 作为源模型和目标模型。  

- 32 × 32：从32层ResNet迁移到9层VGG
- 224 × 224：从34层ResNet迁移到18层ResNet
###元网络架构
在所有实验中，元网络的结构为：每对候选对 (m,n)∈C为一层全连接网络。元网络以源网络第 m 层的全局平均池化特征为输入，输出 wm,nc和 λm,n。通道加权参数 w使用 softmax 激活产生，满足 ∑cwm,nc=1；层之间的迁移 λ则使用max(0,min(6,x))也就是 ReLU6，保证其非负性并避免值太大。  
几种层匹配策略对比如图所示：  
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\结构图.png)  
> ResNet32（左）和 VGG9（右）。 (d) 学习后层间传输量 λm,n。线宽表示转移量。当 λm,n 小于 0.1 时，省略这些线。
> 图(d)显示了从学习迁移后，层对之间的迁移量 λm,n。
###比较迁移学习的方案
评估了先前方法的两个手工配置（single, one-to-one）和本文的方法的一个新的配置（all-to-all）：  
**（a）**single：使用源模型的最后一个特征层和目标模型中具有相同空间分辨率的层作为一对；  
**（b）**one-to-one：将每个下采样层前空间分辨率相同的层作为一对；  
**（c）**all-to-all：使用每个下采样层前的所有层对。对于不同空间大小的特征匹配，使用双线性插值。  

- **下采样**实际上就是缩小图像,主要目的是为了使得图像符合显示区域的大小,生成对应图像的缩略图。
	- 作用：  
		- 一是减少计算量,防止过拟合;
		- 二是增大感受野,使得后面的卷积核能够学到更加全局的信息
###各种目标任务的评估
首先评估学习转移什么what（L2Tw）而不学习转移到哪里的效果。为此，使用传统的手工匹配配置，单个single和一对一one-to-one。对于表中报告的大多数情况，与未加权特征匹配(FM) 相比，L2T-w 提高了目标任务的性能。  
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\正确率.png)  
接下来，不再使用手工制作的匹配层对，而是从图c所示的所有匹配对开始学习从哪里开始传输。论文中提出的最终方案，学习转移什么和在哪里（L2T-ww），与手工匹配（L2T-w）相比，通常会显着提高性能。L2T-ww 在表中报告的所有情况下都实现了最佳精度。  
图 (d) 显示了从 TinyImageNet 到 STL-10 的学习迁移后，层对之间的迁移量 λm,n。如图所示，我们的方法将知识转移到目标模型中的更高层：λ2,5 = 1.40，λ1.5 = 2.62，λ3,4 = 2.88，λ2,4 = 0.74。除了 λ1,2 = 0.21 之外，其他对的数量 λm,n 都小于 0.1。显然，通过手工调优找到这些匹配是很困难的，这证明我们学习转移到哪里的方法是有用的。  

###有限数据机制的实验  
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\少量标注.png)  

当目标任务具有少量用于训练的标记样本时，迁移学习会更加有效。为了在这种有限数据场景下评估我们的方法 (L2T-ww)，通过减少样本数量使用 CIFAR-10 作为目标任务数据集。作者为每个类使用 N ∈ {50, 100, 250, 500, 1000} 训练样本，并比较从头开始学习、LwF、A T、LwF+A T 和 L2T-ww 的性能。结果如图所示。它们表明，当目标数据集的体积较小时，与其他基线相比，作者的改进取得了显着的进步。  
观察到每类只需要 50 个样本即可达到与每类 250 个样本的 LwF 相似的精度。  
###多源传输实验  
多源迁移由于涉及到多个数据集，网络学习过程可能受到不同来源数据的扰动。作者测试的结果同样表明了本方法的有效性。    
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\多源.png)  
###可视化  
![1](E:\MarkDown笔记\学习迁移什么和迁移到哪图片\可视化.png)  
如图所示，使用 L2T-w 时，包含特定任务对象（鸟或狗）的像素被激活得更多，而背景像素被激活得更少。这意味着权重 wm,n 使源模型的知识更加特定于任务，因此它可以改进迁移学习。


##结论 
作者提出了一种基于元学习的迁移方法，它可以根据任务和架构选择性地迁移知识。作者的方法通过使用元网络识别要传输的内容和位置，为学习目标任务传输更重要的知识。为了学习元网络，作者设计了一个有效的元学习方案，它需要在内环过程中执行几个步骤。通过这样做，作者联合训练目标模型和元网络。
